{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update TDWG documents metadata\n",
    "\n",
    "Author: Steve Baskauf - 2022-05-27\n",
    "\n",
    "Version: 0.1\n",
    "\n",
    "This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "\n",
    "This script is a companion to the other script that updates the vocabularies metadata and should be run after it is finished and any new list of terms documents have been created.\n",
    "\n",
    "After some use and testing, this will get turned from a Jupyter notebook to a stand-alone script. \n",
    "\n",
    "NOTE: Creating new documents metadata depends on two YAML configuration files (`authors_configuration.yaml` and `document_configuration.yaml`) that are located in a subdirectory whose name is formed from the path of the document name. (For example, `http://rs.tdwg.org/ac/doc/subtype/` is in the directory `ac_doc_subtype`.) To generate them based on existing documents or to use an existing document as a template for a new document, there are some cells at the end of the script that can be used. These files will be generated in the same directory as this script and should be moved to a subdirectory with an appropriate name.\n",
    "\n",
    "NOTE: the `doc_modified` value is ignored as it will be replaced by the `version_date` in the general configuration file.\n",
    "\n",
    "When the script is run to update existing documents, the YAML files aren't needed unless changes are being made. In the absence of those configuration files, the previous data will be used. Any values provided in the YAML file will be used. If a value is missing, the previous value will be used.\n",
    "\n",
    "The `general_configuration.yaml` file needs to be updated every time and is therefore located in the same directory as this script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Update TDWG documents metadata\n",
    "\n",
    "# Author: Steve Baskauf - 2022-05-27\n",
    "# Version: 0.2\n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "\n",
    "# This script is a companion to the other script that updates the vocabularies metadata and \n",
    "# should be run after it is finished and any new list of terms documents have been created.\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration and function definitions \n",
    "# ---------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "from os.path import exists\n",
    "import datetime\n",
    "\n",
    "def csv_read(path, **kwargs):\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "            \n",
    "def iso_iime(offset):\n",
    "    current_time = datetime.datetime.now()\n",
    "    return current_time.strftime(\"%Y-%m-%dT%H:%M:%S\") + offset\n",
    "\n",
    "# --------------------------------\n",
    "# Load document data\n",
    "\n",
    "#If the document already exists, its data is retrieved from current documents CSV. \n",
    "# A `document_configuration.yaml` file provides new data, which replaces any existing data \n",
    "# or is used to create a new record.\n",
    "# --------------------------------\n",
    "repo_path = '../../'\n",
    "new_accessUrl = ''\n",
    "general_config_path = 'general_configuration.yaml'\n",
    "\n",
    "# Load data from a YAML configuration file.\n",
    "if exists(general_config_path):\n",
    "    with open(general_config_path) as file_object:\n",
    "        config_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    sys.exit('Must have a general_configuration.yaml file for this script to operate.')\n",
    "\n",
    "utc_offset = config_data['utcOffset']\n",
    "\n",
    "# Generate the data subdirectory path from the document IRI in the configuration file\n",
    "subdirectory_path = '_'.join(config_data['docIri'].split('/')[3:-1]) + '/'\n",
    "    \n",
    "#print(json.dumps(config_data, indent=2))\n",
    "doc_config_path = subdirectory_path + 'document_configuration.yaml'\n",
    "#print(doc_config_path)\n",
    "author_config_path = subdirectory_path + 'authors_configuration.yaml'\n",
    "#print(author_config_path)\n",
    "\n",
    "# Set the value of any missing config keys to empty string\n",
    "for key in config_data.keys():\n",
    "    # Empty YAML values are read in as a None object.\n",
    "    if config_data[key] is None:\n",
    "        config_data[key] = ''\n",
    "\n",
    "version_date = config_data['versionDate']\n",
    "doc_iri = config_data['docIri']\n",
    "\n",
    "standard_iri = config_data['standardIri']\n",
    "\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs.csv')\n",
    "\n",
    "# Find the row index if the document already exists\n",
    "row_matches = current_docs_df.index[current_docs_df['current_iri']==doc_iri].tolist()\n",
    "if len(row_matches) == 0:\n",
    "    print('Document IRI not found in existing data.')\n",
    "    new_document = True\n",
    "elif len(row_matches) > 1:\n",
    "    sys.exit('Multiple rows match the document IRI:' + str(row_matches))\n",
    "else:\n",
    "    row_index = row_matches[0]\n",
    "    new_document = False\n",
    "\n",
    "    # .squeeze() turns a single-row or column dataframe into a series.\n",
    "    # See https://stackoverflow.com/questions/50575802/convert-dataframe-row-to-dict\n",
    "    # and https://www.w3resource.com/pandas/dataframe/dataframe-squeeze.php\n",
    "    row_data = current_docs_df[current_docs_df['current_iri']==doc_iri].squeeze().to_dict()\n",
    "#print('Existing row data:')\n",
    "#print(json.dumps(row_data, indent=2))\n",
    "#print()\n",
    "\n",
    "# Try to load new document data from a configuration file.\n",
    "if exists(doc_config_path):\n",
    "    with open(doc_config_path) as file_object:\n",
    "        new_row_data = yaml.safe_load(file_object)\n",
    "    \n",
    "    # Need to stash any new accessUrl that is provided\n",
    "    if new_row_data['accessUrl'] != None: # Empty YAML values are read in as a None keyword.\n",
    "        new_accessUrl = new_row_data['accessUrl']\n",
    "\n",
    "    # For new documents, the data from the file is used as the initial record.\n",
    "    if new_document:\n",
    "        row_data = new_row_data\n",
    "    # For existing documents, any new data replaces the existing data.\n",
    "    else:\n",
    "        for key in new_row_data.keys():\n",
    "            if new_row_data[key] != None: # Empty YAML values are read in as a None keyword.\n",
    "                row_data[key] = new_row_data[key]\n",
    "    \n",
    "else:\n",
    "    # If the document is new but there isn't a config file, there are no data to work with for the document\n",
    "    if new_document:\n",
    "        sys.exit('New documents must have a document_configuration.yaml file.')\n",
    "\n",
    "# Replace any existing doc_modified date with the new version date\n",
    "row_data['doc_modified'] = version_date\n",
    "\n",
    "#print('New row data:')\n",
    "#print(json.dumps(row_data, indent=2))\n",
    "\n",
    "# --------------------------------\n",
    "# Write new data to the current documents CSV\n",
    "# --------------------------------\n",
    "\n",
    "if new_document: # If it's a new document, the row data gets added to the end of the DataFrame\n",
    "    # Constructs a one-row DataFrame from a list containing a single dict, then concatenates it to the end\n",
    "    # of the existing DataFrame.\n",
    "    current_docs_df = pd.concat([current_docs_df, pd.DataFrame([row_data])])\n",
    "else: # The new values of the row cells replace the old one.\n",
    "    for key in row_data:\n",
    "        current_docs_df.at[row_index, key] = row_data[key]\n",
    "\n",
    "current_docs_df.to_csv(repo_path + 'docs/docs.csv', index = False)\n",
    "\n",
    "# --------------------------------\n",
    "## Update the documents versions metadata\n",
    "# --------------------------------\n",
    "\n",
    "# Generate a new version for the document based on the current document IRI and version_date.\n",
    "doc_version_iri = row_data['current_iri'] + version_date\n",
    "\n",
    "# Load versions list and find most recent version if not a new document.\n",
    "versions_list_df = csv_read(repo_path + 'docs/docs-versions.csv')\n",
    "if not new_document:\n",
    "    matching_versions = versions_list_df[versions_list_df['current_iri']==doc_iri]\n",
    "    matching_versions = matching_versions.sort_values(by=['version_iri'], ascending=[False])\n",
    "    most_recent_version_iri = matching_versions.iat[0, 1]\n",
    "#print(most_recent_version_iri)\n",
    "\n",
    "# Update the list of document versions in the docs folder\n",
    "version_row_data = {'current_iri': row_data['current_iri'], 'version_iri': doc_version_iri}\n",
    "versions_list_df = pd.concat([versions_list_df, pd.DataFrame([version_row_data])])\n",
    "\n",
    "versions_list_df.to_csv(repo_path + 'docs/docs-versions.csv', index = False)\n",
    "\n",
    "# Wrangle current document metadata row dictionary to match the versions metadata column headers\n",
    "versions_data = copy.deepcopy(row_data)\n",
    "\n",
    "del versions_data['doc_created']\n",
    "del versions_data['doc_modified']\n",
    "versions_data['version_issued'] = version_date\n",
    "versions_data['version_iri'] = doc_version_iri\n",
    "versions_data['mediaType'] = config_data['mediaType']\n",
    "\n",
    "# Update the document versions metadata in the docs-versions folder\n",
    "versions_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions.csv')\n",
    "versions_metadata_df = pd.concat([versions_metadata_df, pd.DataFrame([versions_data])])\n",
    "versions_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions.csv', index = False)\n",
    "\n",
    "# Update the versions replacements unless the document is new\n",
    "if not new_document:\n",
    "    versions_replacements_df = csv_read(repo_path + 'docs-versions/docs-versions-replacements.csv')\n",
    "    replacement_row_data = {'replacing_document': doc_version_iri, 'replaced_document': most_recent_version_iri}\n",
    "    versions_replacements_df = pd.concat([versions_replacements_df, pd.DataFrame([replacement_row_data])])\n",
    "    versions_replacements_df.to_csv(repo_path + 'docs-versions/docs-versions-replacements.csv', index = False)\n",
    "\n",
    "# --------------------------------\n",
    "# Update the access URLs and media types\n",
    "# --------------------------------\n",
    "\n",
    "# Load format information\n",
    "formats_metadata_df = csv_read(repo_path + 'docs/docs-formats.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Look for the previously used format information for this doc\n",
    "if not new_document:\n",
    "    old_accessUrl = formats_metadata_df.loc[formats_metadata_df.doc_iri == doc_iri, 'accessUri'].values[0]\n",
    "    old_mediaType = formats_metadata_df.loc[formats_metadata_df.doc_iri == doc_iri, 'mediaType'].values[0]\n",
    "    \n",
    "#print(old_accessUrl)\n",
    "#print(old_mediaType)\n",
    "\n",
    "# If there is a newly provided access URL and media type for the current document, use it.\n",
    "# Otherwise use the old one.\n",
    "\n",
    "# NOTE: if it's a new document, a new accessUrl must be provided along with the rest of the metadata.\n",
    "# If that isn't done, the script here doesn't handle it and will throw an error later when current_accessUrl\n",
    "# doesn't have a value.\n",
    "if new_accessUrl:\n",
    "    current_accessUrl = new_accessUrl\n",
    "else:\n",
    "    current_accessUrl = old_accessUrl\n",
    "\n",
    "if config_data['mediaType']:\n",
    "    current_mediaType = config_data['mediaType']\n",
    "else:\n",
    "    try:\n",
    "        current_mediaType = old_mediaType\n",
    "    # Handle the case where the creator of a new document doesn't bother to create the format config file\n",
    "    except: # We assume the document is in Markdown if no information is given\n",
    "        current_mediaType = 'text/markdown'\n",
    "        \n",
    "# For pre-existing documents, we try to replace the values of the accessUrl and mediaType, which might change.\n",
    "if not new_document:\n",
    "    # Find the row for the pre-existing document\n",
    "    not_found = False\n",
    "    row_matches = formats_metadata_df.index[formats_metadata_df['doc_iri']==doc_iri].tolist()\n",
    "    if len(row_matches) == 0:\n",
    "        not_found = True # If not previously present, we'll add it as if it were a new document and fix it.\n",
    "    else:\n",
    "        if len(row_matches) > 1:\n",
    "            print('Warning: Multiple rows in the docs-formats.csv file match the document IRI:' + str(row_matches))\n",
    "            row_index = row_matches[0]\n",
    "        else:\n",
    "            row_index = row_matches[0]\n",
    "        # Now make the replacements\n",
    "        formats_metadata_df.at[row_index, 'mediaType'] = current_mediaType\n",
    "        formats_metadata_df.at[row_index, 'accessUri'] = current_accessUrl\n",
    "        \n",
    "# Cases where we need to add a row because the media type wasn't there before  \n",
    "if new_document or not_found:\n",
    "    format_row_data = {'doc_iri': doc_iri, 'mediaType': current_mediaType, 'accessUri': current_accessUrl}\n",
    "    formats_metadata_df = pd.concat([formats_metadata_df, pd.DataFrame([format_row_data])])\n",
    "\n",
    "# Now save the updated table\n",
    "formats_metadata_df.to_csv(repo_path + 'docs/docs-formats.csv', index = False)\n",
    "\n",
    "# Load format information for versions.\n",
    "versions_format_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions-formats.csv')\n",
    "\n",
    "# The previous version usually needs to have it's access URL changed since it's not the current version webpage any more.\n",
    "if not new_document:\n",
    "    # Find the row for the pre-existing document\n",
    "    not_found = False\n",
    "    row_matches = versions_format_metadata_df.index[versions_format_metadata_df['version_iri']==most_recent_version_iri].tolist()\n",
    "    if len(row_matches) == 0:\n",
    "        print('no match found')\n",
    "        not_found = True # If not previously present, we'll add it as if it were a new document and fix it.\n",
    "    else:\n",
    "        if len(row_matches) > 1:\n",
    "            print('Warning: Multiple rows in the docs-versions-formats.csv file match the document IRI:' + str(row_matches))\n",
    "            row_index = row_matches[0]\n",
    "        else:\n",
    "            row_index = row_matches[0]\n",
    "        # Now make the replacement, using the URL provided in the format config file, if it was provided.\n",
    "        # If it wasn't provided, then whatever URL was already there will remain.\n",
    "        if config_data['lastVersionAccessUri']:\n",
    "            versions_format_metadata_df.at[row_index, 'accessUri'] = config_data['lastVersionAccessUri']\n",
    "            \n",
    "# Handle the edge case where the row for the previous document is missing.\n",
    "# Doesn't error trap the case where the old access URI isn't provided, but hey, it's an edge case and be more careful.\n",
    "if not_found:\n",
    "    versions_format_row_data = {'version_iri': most_recent_version_iri, 'mediaType': old_mediaType, 'accessUri': config_data['lastVersionAccessUri']}\n",
    "    versions_format_metadata_df = pd.concat([versions_format_metadata_df, pd.DataFrame([versions_format_row_data])])\n",
    "\n",
    "# For versions, a new row is always added to the file\n",
    "versions_format_row_data = {'version_iri': doc_version_iri, 'mediaType': current_mediaType, 'accessUri': current_accessUrl}\n",
    "versions_format_metadata_df = pd.concat([versions_format_metadata_df, pd.DataFrame([versions_format_row_data])])\n",
    "\n",
    "versions_format_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions-formats.csv', index = False)\n",
    "\n",
    "# --------------------\n",
    "## Update data about authors\n",
    "\n",
    "# Behaviors:\n",
    "# 1. If there is a configuration file, it gets used as-is. \n",
    "# - For new documents, the authors get added. This is also true for docs-roles.csv .\n",
    "# - For existing documents, the data from the config file replaces the existing data for the current doc. \n",
    "#   Also true for docs-roles.csv .\n",
    "# 2. If there is no configuration file, the current doc data is unchanged. The previous author information \n",
    "#    gets used for the new version. No change is made to the docs-roles.csv file.\n",
    "# --------------------\n",
    "\n",
    "# Load existing author data\n",
    "authors_df = csv_read(repo_path + 'docs/docs-authors.csv')\n",
    "roles_df = csv_read(repo_path + 'docs-roles/docs-roles.csv')\n",
    "\n",
    "# Try to load new document data from a configuration file.\n",
    "# For new documents, the data from the YAML file must be used as the initial record.\n",
    "if exists(author_config_path):\n",
    "    # Load the new author data from the YAML file\n",
    "    with open(author_config_path) as file_object:\n",
    "        author_data = yaml.safe_load(file_object)\n",
    "    for author_number in range(len(author_data)):\n",
    "        # Need to add in the document column\n",
    "        author_data[author_number]['document'] = doc_iri\n",
    "        \n",
    "        # Need to turn None values into empty strings\n",
    "        for key in author_data[author_number].keys():\n",
    "            if author_data[author_number][key] == None: # Empty YAML values are read in as a None keyword.\n",
    "                author_data[author_number][key] = ''\n",
    "        \n",
    "    #print(json.dumps(new_author_data, indent=2))\n",
    "    \n",
    "    if not new_document:\n",
    "        # For existing documents, any new data replaces the existing data.\n",
    "        # Remove existing rows where the doc IRI matches, then add in new author data\n",
    "        authors_df = authors_df[authors_df['document']!=doc_iri]\n",
    "        roles_df = roles_df[roles_df['document']!=doc_iri]\n",
    "        \n",
    "    # Write the modified author DataFrame back out to the authors data file\n",
    "    authors_df = pd.concat([authors_df, pd.DataFrame(author_data)])    \n",
    "    authors_df.to_csv(repo_path + 'docs/docs-authors.csv', index = False)\n",
    "    \n",
    "    # The new (or replacement) rows for docs-roles.csv need to be constructed.\n",
    "    roles_list = []\n",
    "    for author in author_data:\n",
    "        roles_dict = {'document': doc_iri, 'contributor_role': author['contributor_role'], 'contributor_literal': author['contributor_literal']}\n",
    "        # Put the author IRI in the column that corresponds to their role\n",
    "        contributor_role_column_header = author['contributor_role'].replace(' ', '_') # column headers don't have spaces\n",
    "        roles_dict[contributor_role_column_header] = author['contributor_iri']\n",
    "        # Perform a check to warn if the author's role isn't one that's already represented in the columns of the CSV\n",
    "        if not contributor_role_column_header in roles_df.columns:\n",
    "            print('WARNING: author', author['contributor_literal'], 'has the role', author['contributor_role'], 'that is not an existing column in the docs-roles.csv file')\n",
    "        roles_list.append(roles_dict)\n",
    "    # Now add the generated rows to the end of the dataframe and save\n",
    "    roles_df = pd.concat([roles_df, pd.DataFrame(roles_list)])    \n",
    "    roles_df.to_csv(repo_path + 'docs-roles/docs-roles.csv', index = False)    \n",
    "        \n",
    "else: # No new author data found, use existing data. The authors of the current documents (docs-authors.csv) are unchanged.\n",
    "    # Load the existing data from the CSV\n",
    "    author_data = []\n",
    "    for index, row in authors_df.iterrows():\n",
    "        # The row is a Pandas series whose items can be referenced by their identifiers (from the column headers)\n",
    "        if row['document']==doc_iri:\n",
    "            row_dict = row.to_dict()\n",
    "            author_data.append(row_dict)\n",
    "            \n",
    "    #print(json.dumps(rows_list, indent=2))\n",
    "    \n",
    "# Create author records for the new version\n",
    "versions_author_metadata_df = csv_read(repo_path + 'docs-versions/docs-versions-authors.csv')\n",
    "\n",
    "# In each row of the new metadata, change the \"document\" column to the \"document-version\" column with a new IRI\n",
    "versions_author_data = []\n",
    "for author_dict in author_data:\n",
    "    del author_dict['document']\n",
    "    author_dict['document_version'] = doc_version_iri\n",
    "    versions_author_data.append(author_dict)\n",
    "\n",
    "# Now add the modified versions author data to the original DataFrame\n",
    "versions_author_metadata_df = pd.concat([versions_author_metadata_df, pd.DataFrame(versions_author_data)])\n",
    "versions_author_metadata_df.to_csv(repo_path + 'docs-versions/docs-versions-authors.csv', index = False)\n",
    "\n",
    "# --------------------\n",
    "## Update standards components with doc information\n",
    "\n",
    "# The code in this section will generate a new version of the standard if the version doesn't already exist.\n",
    "# NOTE: It will not generate a new standard from scratch. That can be done manually or using the \n",
    "# vocabularies update script.\n",
    "\n",
    "# Any changes to the standard description or citation must be done manually.\n",
    "# --------------------#\n",
    "\n",
    "# Determine whether the standard had already been updated to a new version or not\n",
    "std_df = csv_read(repo_path + 'standards/standards.csv')\n",
    "\n",
    "# Find the row index for the existing standard. \n",
    "# NOTE: The standard record must exist and there must only be one row for it. New standards must be added manually.\n",
    "row_matches = std_df.index[std_df['standard']==standard_iri].tolist()\n",
    "row_index = row_matches[0]\n",
    "\n",
    "# Find the standard_modified value for the existing standard\n",
    "std_modified = std_df.at[row_index, 'standard_modified']\n",
    "\n",
    "# Check whether the last standard_modified value is the same as the versionDate in the config file\n",
    "if std_modified != config_data['versionDate']:\n",
    "    std_version_iri = standard_iri + '/version/' + config_data['versionDate']\n",
    "\n",
    "    # The standard has been updated to a new version\n",
    "    # Update the standard_modified value in the standards.csv file\n",
    "    std_df.at[row_index, 'standard_modified'] = config_data['versionDate']\n",
    "    # Change the document_modified value to the current date\n",
    "    std_df.at[row_index, 'document_modified'] = iso_iime(utc_offset)\n",
    "    std_df.to_csv(repo_path + 'standards/standards.csv', index = False)\n",
    "    \n",
    "    # Add a new row to the standards-versions.csv file in the standards folder\n",
    "    std_version_df = csv_read(repo_path + 'standards/standards-versions.csv')\n",
    "    std_version_row_data = {'version': std_version_iri, 'standard': standard_iri}\n",
    "    std_version_df = pd.concat([std_version_df, pd.DataFrame([std_version_row_data])])\n",
    "    std_version_df.to_csv(repo_path + 'standards/standards-versions.csv', index = False)\n",
    "\n",
    "    # Add a new row to the standards-versions.csv file in the standards-versions folder\n",
    "    std_version_df = csv_read(repo_path + 'standards-versions/standards-versions.csv')\n",
    "\n",
    "    # Find the most recent version of the standard\n",
    "    std_version_sort_df = std_version_df[std_version_df['standard']==standard_iri].copy()\n",
    "    std_version_sort_df = std_version_sort_df.sort_values(by=['version'], ascending=False)\n",
    "    # Get the index of the most recent version\n",
    "    most_recent_version_index = std_version_sort_df.index[0]\n",
    "    #print(most_recent_version_index)\n",
    "    most_recent_version_iri = std_version_sort_df.at[most_recent_version_index, 'version']\n",
    "    #print(most_recent_version_iri)\n",
    "\n",
    "    # Edit the most recent version row\n",
    "    std_version_df.at[most_recent_version_index, 'standard_status'] = 'superseded'\n",
    "    std_version_df.at[most_recent_version_index, 'document_modified'] = iso_iime(utc_offset)\n",
    "\n",
    "    # Add a new row for the new version\n",
    "    # Convert the most recent row into a dictionary\n",
    "    std_version_row_data = std_version_sort_df.loc[most_recent_version_index].to_dict()\n",
    "    # Change the version IRI to the new version IRI\n",
    "    std_version_row_data['version'] = std_version_iri\n",
    "    # Change the status to 'recommended'\n",
    "    std_version_row_data['standard_status'] = 'recommended'\n",
    "    # Change the document_modified date to the current date\n",
    "    std_version_row_data['document_modified'] = iso_iime(utc_offset)\n",
    "    # Change the version_issued date to the version date in the config file\n",
    "    std_version_row_data['version_issued'] = config_data['versionDate']\n",
    "    # Add the new row to the DataFrame\n",
    "    std_version_df = pd.concat([std_version_df, pd.DataFrame([std_version_row_data])])\n",
    "    # Save the DataFrame to the CSV file\n",
    "    std_version_df.to_csv(repo_path + 'standards-versions/standards-versions.csv', index = False)\n",
    "\n",
    "    # Add a row to the standards-versions-replacements.csv file\n",
    "    std_version_replacements_df = csv_read(repo_path + 'standards-versions/standards-versions-replacements.csv')\n",
    "    std_version_replacements_row_data = {'replacing_standard_version': std_version_iri, 'replaced_standard_version': most_recent_version_iri}\n",
    "    std_version_replacements_df = pd.concat([std_version_replacements_df, pd.DataFrame([std_version_replacements_row_data])])\n",
    "    std_version_replacements_df.to_csv(repo_path + 'standards-versions/standards-versions-replacements.csv', index = False)\n",
    "\n",
    "# Update the standard and standard version parts to include the new version\n",
    "\n",
    "if new_document:\n",
    "    # Load existing standards data\n",
    "    stds_parts_df = csv_read(repo_path + 'standards/standards-parts.csv')\n",
    "    \n",
    "    # Add a new row for the new document\n",
    "    stds_parts_row_data = {'standard': standard_iri, 'part': doc_iri, 'rdf_type': 'foaf:Document'}\n",
    "    stds_parts_df = pd.concat([stds_parts_df, pd.DataFrame([stds_parts_row_data])])\n",
    "\n",
    "    stds_parts_df.to_csv(repo_path + 'standards/standards-parts.csv', index = False)\n",
    "\n",
    "# Load existing standards versions data\n",
    "stds_version_parts_df = csv_read(repo_path + 'standards-versions/standards-versions-parts.csv')\n",
    "\n",
    "# Need to remove the obsolete version of this doc that was already assigned to this version of the standard.\n",
    "if not new_document:\n",
    "    stds_version_parts_df = stds_version_parts_df[~((stds_version_parts_df['part']==most_recent_version_iri) & (stds_version_parts_df['standard_version']==standard_iri + '/version/' + version_date))]\n",
    "\n",
    "# Add a new row for the new document version\n",
    "stds_version_parts_row_data = {'standard_version': standard_iri + '/version/' + version_date, 'part': doc_version_iri}\n",
    "stds_version_parts_df = pd.concat([stds_version_parts_df, pd.DataFrame([stds_version_parts_row_data])])\n",
    "\n",
    "stds_version_parts_df.to_csv(repo_path + 'standards-versions/standards-versions-parts.csv', index = False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run the next cell unless necessary\n",
    "\n",
    "The cell can be used to generate template YAML configuration files from existing rows in the table, but most times you won't need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "from os.path import exists\n",
    "\n",
    "def csv_read(path, **kwargs):\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "    \n",
    "repo_path = '../../'\n",
    "general_config_path = 'general_configuration.yaml'\n",
    "\n",
    "# Load data from a YAML configuration file.\n",
    "if exists(general_config_path):\n",
    "    with open(general_config_path) as file_object:\n",
    "        config_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    sys.exit('Must have a general_configuration.yaml file for this script to operate.')\n",
    "\n",
    "# The doc_iri determines the row of the table to be used to generate the sample\n",
    "doc_iri = config_data['docIri']\n",
    "\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs.csv')\n",
    "\n",
    "# Find the row index if the document already exists\n",
    "row_matches = current_docs_df.index[current_docs_df['current_iri']==doc_iri].tolist()\n",
    "if len(row_matches) == 0:\n",
    "    print('Document IRI not found in existing data.')\n",
    "    new_document = True\n",
    "elif len(row_matches) > 1:\n",
    "    sys.exit('Multiple rows match the document IRI:' + str(row_matches))\n",
    "else:\n",
    "    row_index = row_matches[0]\n",
    "    new_document = False\n",
    "\n",
    "    # .squeeze() turns a single-row or column dataframe into a series.\n",
    "    # See https://stackoverflow.com/questions/50575802/convert-dataframe-row-to-dict\n",
    "    # and https://www.w3resource.com/pandas/dataframe/dataframe-squeeze.php\n",
    "    row_data = current_docs_df[current_docs_df['current_iri']==doc_iri].squeeze().to_dict()\n",
    "\n",
    "with open('document_configuration.yaml', 'w', encoding = \"utf-8\") as file_object:\n",
    "    dump = yaml.dump(row_data, allow_unicode=True, sort_keys=False)\n",
    "    file_object.write(dump)\n",
    "\n",
    "# Data for authors\n",
    "current_docs_df = csv_read(repo_path + 'docs/docs-authors.csv')\n",
    "\n",
    "rows_list = []\n",
    "for index, row in current_docs_df.iterrows():\n",
    "    # The row is a Pandas series whose items can be referenced by their identifiers (from the column headers)\n",
    "    if row['document']==doc_iri:\n",
    "        row_dict = row.to_dict()\n",
    "        del row_dict['document']\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "with open('authors_configuration.yaml', 'w', encoding = \"utf-8\") as file_object:\n",
    "    #dump = yaml.dump(rows_list)\n",
    "    dump = yaml.dump(rows_list, allow_unicode=True, sort_keys=False)\n",
    "    dump = dump.replace('\\n-', '\\n\\n-') # Insert extra newline between records\n",
    "    file_object.write(dump)\n",
    "\n",
    "#print(json.dumps(rows_list, indent =2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f96c65e2c1d4fcba82e9525c1be2fd15c6a14102f9c31bd3457b5f48c526190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
