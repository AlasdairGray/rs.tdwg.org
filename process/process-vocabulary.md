# Processing a vocabulary spreadsheet

**Title:** Processing a vocabulary spreadsheet

**Date Modified:** 2024-03-01

**Part of TDWG Standard:** Not part of any standard

**Abstract:** Once vocabulary developers have defined terms using a spreadsheet, the data in that spreadsheet can be processed into other forms used to generate human and machine readable representations of the data in the spreadsheet. This document provides information about how to use scripts to generate those representations.

**Contributors:** Steve Baskauf (TDWG Technical Architecture Group, TDWG Audubon Core Maintenance Group, TDWG Darwin Core Maintenance Group)

# Table of Contents

[1 Introduction](#user-content-1-introduction)

[2 Generating necessary CSV files from the hand-generated CSV file](#user-content-2-generating-necessary-csv-files-from-the-hand-generated-csv-file)

[3 Creating a column header mapping file](#user-content-3-creating-a-column-header-mapping-file)

[4 Term list build script](#user-content-4-build-script-for-human-readable-document-listing-terms-and-their-metadata)

[5 Managing documents metadata](#5-managing-documents-metadata-via-python-script)

[6 Generating JSON-LD for controlled vocabularies](#user-content-6-generating-json-ld-for-controlled-vocabularies)

# 1 Introduction

## 1.1 RFC 2119 statement

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

Use of 2119 keywords is not an indication that compliance is required by any TDWG standard. Rather, it is an indication that the associated software will not function as designed if the user does not comply with the requirements of this document.

## 1.2 Audience

This document is intended for those who are responsible for maintaining the TDWG infrastructure. It can also be used by anyone who is developing a vocabulary and wants to generate draft term list documents from a hand-generated CSV file.

## 1.3 Background

The [TDWG Standards Documentation Specification](http://rs.tdwg.org/sds/doc/specification/) (SDS) indicates that all human and machine readable representations of vocabulary components should provide the same data. That can be achieved by using a script to generate those representations from a common data source: CSV files generated from a [basic hand-generated CSV file created by the vocabulary developers](create-vocabulary.md). The process is similar regardless of whether it is a new vocabulary or if modifications are being made to an existing vocabulary.

![generation of metadata tables](images/table-generation.png)

A Python script uses the data present in the hand-generated CSV files to generate several CSV files that contain all of the metadata required by the SDS. The data are used to generate specific term versions and to update the current terms by automatically adding some fields that are generated by the script. The script also links the versions to the current terms in a join table.

![generation of machine readable metadata](images/machine-readable-mapping.png)

Data in the generated current terms CSV file is used with a mapping table to generate machine-readable metadata about the terms. The mapping table is hand-edited as necessary when the vocabulary is first created and relates the header names in the current terms CSV file to the abbreviated property IRIs used in the machine readable representation.

![generation of human readable document](images/human-readable-mapping.png)

### 1.3.1 Human readable document listing terms

The current terms CSV file can also be used along with a Python build script to create a human readable document listing terms and their metadata. 

**Note:** There is a distinction between this document listing terms and a "term list" document. *Term list* is a technical term defined in [section 3.3.3 of the SDS](http://rs.tdwg.org/sds/doc/specification/) denoting a list of terms incorporated into a vocabulary that share a common namespace. Therefore, a "term list" document is a document that describes all of the terms included in a term list. The document listing terms that is described here may or may not be the same as a "term list" document since it can include terms from a single namespace or terms from an entire vocabulary that consists of multiple term lists. Documents listing terms in a vocabulary are typically called "List of Terms" documents.

During the initial vocabulary development process, a List of Terms build script can be used to generate drafts for review. Re-running the build script will cause changes or corrections made to the hand generated CSV file to be reflected in a revised document listing terms. Typically, the drafts are managed in a branch, since making the changes to the "master" branch could potentially make the drafts go "live".

## 1.4 How to use this document

This document explains the steps for processing a hand-generated CSV file using a Python script. NOTE: term deprecations cannot be carried out using this workflow and they require a number of special steps. See the [notes at the start of the detailed Jupyter notebook](process_rs_tdwg_org.ipynb) for specific steps that are necessary for term deprecations.

### 1.4.1 Requirements

To carry out the process described in this document, you need:
- to know how to use Git and GitHub. The simplest way to carry out the necessary operations is to download the [GitHub Desktop client](https://desktop.github.com/). An introduction to Git and GitHub is [here](http://vanderbi.lt/github).
- to know how to edit a YAML configuration file using a text editor.
- to know how to run a Python script, and have Python installed on your local computer.

### 1.4.2 Processing script and configuration file

The script [process.py](https://github.com/tdwg/rs.tdwg.org/blob/master/process/process.py) will update multiple namespaces within a single vocabulary at one time. It uses a YAML configuration file, `config.yaml`, to know where the data are located and how to process the CSV files. An example configuration file is [here](https://github.com/tdwg/rs.tdwg.org/blob/da380f5baffbed3a035b64d3b8193e14e71933ab/process/config.yaml).

There are also two Python scripts in Jupyter notebooks that were used to develop the script and formerly used to do the processing. They are no longer maintained, but contain a lot of comments that might help in understanding what the script does. They may also be useable for term deprecations. They are:

1. The [simplified processing script](simplified_process_rs_tdwg_org.ipynb) presupposes no knowledge of Python and will work for most term additions and changes in existing standards and for creating simple vocabularies or term lists, including controlled vocabularies. **You MUST NOT use this script for term deprecations.**
2. Because this script is not designed for use by the general public, it has limited error trapping. In cases where results are not as expected, or where unusual changes such as term deprecations are required, the [full processing script](process_rs_tdwg_org.ipynb) SHOULD be used. This script contains the same code as the simplified script, but separates the code among more cells and provides more feedback in the form of print statements. **Note on 2024-03-01: Since this script was written, the processing script has been significantly modified. You should not assume that the full processing script notebook is usable without modification.**

We really should not be deprecating terms anyway, so there should be only rare cases where using the full processing script is necessary.

### 1.4.3 General workflow

1. Clone the [rs.tdwg.org](https://github.com/tdwg/rs.tdwg.org) repository to your local drive.
2. Create a new branch of the repository.
3. Place the hand-generated CSV files in some subdirectory of the `process` directory of the repository. There are existing directories called `ac-revisions` for Audiovisual Core and `dwc-revisions` for Darwin Core. There are [example spreadsheets](https://github.com/tdwg/rs.tdwg.org/tree/master/process/example-spreadsheets) that can be used as an example. For more information, see the [instructions for creating a vocabulary](https://github.com/tdwg/rs.tdwg.org/blob/master/process/create-vocabulary.md#user-content-3-details-and-examples). NOTE: when updating terms in an existing vocabulary, it is best to copy cells from the existing primary metadata CSV file (the one that shares the name of the directory) to avoid typographical errors that would result in unwanted changes to the term metadata.
4. Open the `config.yaml` file in a text editor. The existing file will have values from previous updates, which you can edit and use as a template.
5. Enter the general configuration settings and settings for each of the namespaces to be updated. There are detailed comments in the YAML file to guide you. You can also look in recent existing revisions files for examples if you are unsure about how to edit the file.
6. For changes to existing vocabularies or standards, the vocabulary or standards-level metadata will be taken from the previous version. 
If this is a new vocabulary, edit the appropriate files in the `process/files_for_new` directory of the repository. See the details below.
7. Before running the script, make a commit that you can go back to if things don't go as anticipated. Run the script. 
8. After running the script, carefully examine the diffs for the changed files to make sure that they make sense. This can easily be done using the GitHub Desktop client. If something did not go as planned, discard the changes to go back to the previous commit.  If really bad things happen and you want to start over, commit the changes, then delete the branch you created.
9. When term changes have been ratified, the ratification decision needs to be recorded. First, create a new decision IRI in the [decisions.csv](https://github.com/tdwg/rs.tdwg.org/blob/master/decisions/decisions.csv) table. Then add this decision local name along with the term IRI of every changed term into the [decision-links.csv](https://github.com/tdwg/rs.tdwg.org/blob/master/decisions/decisions-links.csv) file. NOTE: this needs to be done before the next step in order for the decision to be included in the metadata for the term in the human-readable document. In the case where ratification has occurred and this step is done, the branch will have to be merged into the master in order for the changed to be picked up in the next step.
10. If the changes look sensible, then you can run a script to generate a human readable document listing terms and their metadata (See section 4 below). Revisions made based on drafts of this document should be made to the hand-generated CSV file. That revised CSV file should then be reprocessed in a new branch and the human readable document regenerated.
12. Once the human-readable term list documents have been generated, the metadata about them must be added or updated in the `rs.tdwg.org` repo. See Section 5 below for details. Push the changes to GitHub and create a pull request to merge the changes from the branch into the master.
13. In production, merging the changes into the master branch rebuilds and deploys the server that controls redirects and machine-readable metadata at http://rs-test.tdwg.org . After testing to make sure that its behavior is appropriate, a new release of the rs.tdwg.org repository should be made. That triggers deployment to the "real" http://rs.tdwg.org server and the changes should be "live". 

# 2 Generating necessary CSV files from the hand-generated CSV file

There are several steps necessary to generate all of the metadata related to term additions or changes. A new version of the term is usually created, then the metadata record for the current term will be created (if the term is new) or modified (if the term is revised). The new version is then linked to its corresponding current term. 

![TDWG metadata model](https://raw.githubusercontent.com/tdwg/vocab/master/tdwg-standards-hierarchy-2017-01-23.png)

Ratification of a term addition or change triggers new versions at all of the higher levels in the TDWG standards hierarchy. New term versions trigger new term list versions. New term list versions trigger new vocabulary versions and new vocabulary versions trigger new standards versions. For more information about versioning of TDWG standards, see [Section 2.3 of the TDWG Standards Documentation Specification](http://rs.tdwg.org/sds/doc/specification/).

The next section describes how to configure and run the processing script. Sections 2.2 through 2.5 are informational and describe how the script changes metadata in various categories. These sections may be helpful when examining the diffs to see if the changes that were made make sense.

## 2.1 Setup

After the repo has been set up on your local drive (see 1.4.3 General workflow above), you MUST edit the `config.yaml` file to reflect the term lists (i.e. namespaces) you are creating or changing. If you are creating a new term vocabulary, you MUST modify files in the `process/files_for_new` directory as well. 

The script is designed to handle the creation of simple vocabularies or maintenance of existing vocabularies through a streamlined process. However, there are two more complicated circumstances that will require manual editing of files. If you are creating a new vocabulary and the hand-edited CSV file contains columns for additional properties beyond those required by the Standards Documentation Specification, you MUST manually edit the column header mapping file. This is discussed in section 3 below. If you are creating a new vocabulary that contains borrowed terms from multiple namespaces (as in the example spreadsheet [complex-vocabulary.csv](example-spreadsheets/complex-vocabulary.csv)), the rows for each namespace MUST be copied and pasted into separate CSV files (one for each namespace). Each of these separate CSV files MUST be described as separate JSON objects in the `namespaces` array of the JSON configuration file.

### 2.1.1 Editing the configuration YAML file

Each setting in the configuration file will be discussed separately below.

```
"date_issued": "2020-06-15"
```
The date issued is assigned as the date of issue for all versions and the modification date for current resources. It is also appended to version IRIs. The date SHOULD fall between the current date and the latest date on which all changes included in the version were ratified or completed. Typically, this will be the date of the approval by the Executive (if approval was needed for the change), but there isn't actually any rule that says it has to be. The main purpose of the date issued is to allow the versions to be ordered.

```
"local_offset_from_utc": "-05:00"
```
This SHOULD be the UTC offset for the computer running the script (i.e. the appropriate offset for values produced the python method `datetime.datetime.now()`).

```
"vocab_type": "2"
```
This value is only relevant when new term lists or vocabularies are created. It does nothing when existing terms are changed. It controls the template column mapping files copied into the current terms and versions directories. Those template mapping files have names ending in `-mappings` and are located [here for current terms](files_for_new/current_terms) and [here for versions](files_for_new/versions). The three categories:
1 for simple vocabulary, 2 for simple controlled vocabulary, 3 for c.v. with broader hierarchy, correspond to the three template spreadsheet types [here](example_spreadsheets). If additional property columns are added beyond those already present in the template spreadsheets, select the most appropriate category, then edit the template mapping file as described in section 3 below.

```
list_of_terms_iri
```
Permanent IRI for the list of terms document that is associated with this vocabulary.

```
standard
```
IRI of containing standard. Examples listed in the configuration file notes.


The following settings must be made for each term list (corresponding to a namespace) that is being changed by a separate CSV file.

```
"namespaceUri": "http://rs.tdwg.org/dwc/doe/"
```
For existing TDWG term lists and borrowed terms, the namespace IRI MUST be the one assigned by the existing standard. For proposed new term lists minted by TDWG, the namespace MUST conform to the [conventional TDWG IRI patterns](https://github.com/tdwg/rs.tdwg.org#2-iri-patterns).

```
pref_namespace_prefix: dwcdoe
```
Standard namespace abbreviation for the namespace IRI.

```
"database": "degreeOfEstablishment"
```
The database name is used to generate names for associated directories within the rs.tdwg.org repository and as the root for file names within those folders. The file name SHOULD be descriptive and lower camelCase is RECOMMENDED. It MUST NOT contain spaces. Terms that are borrowed SHOULD follow the naming convention established for Darwin and Audubon Cores, i.e. `descriptiveName-for-vocab`, where `vocab` is an abbreviation for the borrowing vocabulary. See examples [here](https://github.com/tdwg/rs.tdwg.org). Do not append `-versions` to this name -- the versions directory will be located or created automatically by the script.

```
"borrowed": true
```
MUST be set to `true` if the namespace is not issued by TDWG in the `http://rs.tdwg.org/` subdomain. MUST be set to `false` if the namespace is controlled by TDWG.

```
"new_term_list": false
```
MUST be set to `true` if it is a new term list that has never been processed before. Note that there are a number of files that must be set up for new term lists. See Section 2.1.2 for details. MUST be set to `false` if this is an existing term list that has been processed at some time in the past.

```
"utility_namespace": false
```
This is generally set to `false` except in the edge case of namespaces that do not have versions like the decisions namespace.


```
"modifications_file_path": "dwc-revisions/dwc-revisions-2021-07-15/dcterms_2021-07-15.csv"
```
This is the path to the CSV containing the hand-edited changes and additions. It is relative to the `process` directory in which the `process.py` script is running.

```
"termlist_uri": ""
```
For TDWG-minted terms, this value SHOULD be the empty string and the termlist IRI will be set to be the same as the namespace IRI. If a value is given for TDWG-minted terms, it MUST be the same as the namespace IRI. When terms are borrowed from other non-TDWG vocabularies to be included within a TDWG vocabulary, an [IRI for the borrowed term list conforming to the term list IRI pattern](https://github.com/tdwg/rs.tdwg.org#3rd-level-iris-denoting-term-lists) MUST be minted. The subdomain MUST be `rs.tdwg.org` and the first level IRI component following the subdomain MUST be the standard component for the vocabulary that is borrowing the terms. The second level IRI component SHOULD be a short, memorable string commonly associated with the borrowed vocabulary. See [this table](../term-lists/term-lists.csv) for examples.

```
label
```
Label used for the term list in machine-readable metadata.

```
description
```
Description of the term list used in machine-readable metadata.

### 2.1.2 Editing the template files for new term lists, vocabularies, and standards

When existing metadata records are updated for term lists, vocabularies, and standards, the basic metadata for those resources (labels, descriptions, and other properties such as preferred namespace abbreviations) are copied from the previous version. Changes to those properties would need to be made manually prior to publishing the data. However, when a new term list, vocabulary, or standard is created, values for those basic metadata properties MUST be provided from template files. Those files are located [here](files_for_new). Follow the patterns in the files while changing the values to those appropriate for the new resource. It is not necessary to provide values for modified or created dates since they will be generated automatically.

When a new resource is created, template files for resources below it in the standards hierarchy MUST also be created. It is not necessary to edit template files at a higher level if the higher-level resource already exists. For example, adding a new vocabulary to the Darwin Core standard would require editing the template `new_vocabulary.csv` and `new_term_list.csv` files, but not the `new_standard.csv` file.

### 2.1.3 Running the processing script for setup

Run the `process.py` script, then check the diffs to make sure that the changes made make sense. 

## 2.2 Generating term versions (informational)

Each current term is related to at least one term version.

![TDWG versions model](https://github.com/tdwg/vocab/raw/master/graphics/version-model.png)

Each time a term's metadata is revised, a new version is created. The term version IRI is formed by appending the date of issue to the term local name. A `hasVersion` relationship is created between the term and its version, and the new version has a `replaces` relationship with the previous version. The metadata defining these relationships are generated by the processing script, and the definition, usage, and notes are copied from the hand-generated CSV file.

## 2.3 Revising current term metadata (informational)

If a term is being revised, its metadata are changed according to the information in the hand-generated CSV file and the last-modified date for that term is updated. If the term is new, its record is created and the last-modified date is set to be the same as the created date. 

## 2.4 Assignment of term versions to a new term list version (informational)

A term list is a group of related terms that share the same namespace part of their IRI. As with all TDWG resources, term lists also have versions. When a term is changed or added, the new term version is added to a new version of the term list (replacing any older version if necessary). If a term is new, it is also added to the existing term list. 

## 2.5 Proliferation of new versions up the hierarchy (informational)

A new term list version is updated in its parent vocabulary version and a new vocabulary version is updated in its parent standard version. A term list is only added to its parent vocabulary if it represents terms in a namespace that is not already represented in the vocabulary. Similarly, vocabularies are only added to a standard if they are new, although new versions of both the vocabulary and standard are recorded.

# 3 Creating a column header mapping file

Because the SDS requires particular properties to be included in term metadata, if the template hand-generated CSV file is used without editing the column headers, a template column header mapping file can be used as well. The column header mapping file only needs to be modified if additional property columns are added to the template CSV file. This may happen if specialty properties are added to the required properties.

Controlled vocabularies contain one or more additional properties that are not found in vocabularies that define properties and classes. That includes the controlled value string and may also include a property to indicate that a value has a `broader` relationship to another concept. So controlled vocabularies should use one of the template column header mapping files designed for controlled vocabularies. Setting the value of `vocab_type` in the configuration section determines whether the mapping template includes mappings for these extra term columns or not. See section 2.1.1 for details.

## 3.1 Modifying the column header mapping file

If additional property columns were added to the hand-generated CSV file, the mapping file in the current terms directory for that term list (i.e. the directory created having the name set as the value of `database` in the configuration section) must be manually edited. The name of the mapping file ends in `-mappings.csv`. 

The order of rows in the mapping file does not matter. The first column (`header`) contains the name of the column header in the hand-generated CSV file. The second column (`predicate`) contains the abbreviated IRI (also known as [CURIE](https://www.w3.org/TR/curie/) or [QName](https://www.w3.org/2001/tag/doc/qnameids)). If the namespace abbreviation of an added row is different from others already present in this column, check the `namespace.csv` file in the same directory to make sure that the abbreviation is already listed. If not, add it to that list of namespace abbreviations and IRIs. The third column, which describes the type of the value in the column, MUST have one of the following strings as its value: `iri`, `language`, `datatype`, or `plain`. For language-tagged strings, the `attribute` column contains the ISO 639-1 language code used in the tag. For strings having a `datatype`, the `attribute` column contains the abbreviated IRI for the datatype. If the column in the CSV file contains an unabbreviated full IRI, there is no value in the `value` column of the mapping table. If the column in the CSV contains the local name part of the IRI, the `value` column contains full namespace IRI to be prepended to the value from column in the CSV. 

It is also possible to generate a fixed value for all rows in the CSV table. See [this page](https://github.com/baskaufs/guid-o-matic/blob/master/use.md#recording-the-column-mappings-from--the-metadata-table-to-rdf-triples) for more details on the format of the mapping file. 

# 4 Build script for human readable document listing terms and their metadata

A document listing terms and their metadata is a Markdown document consisting of two or more parts. The first part is a hand-edited static file that contains the introductory material (header section, introduction, RFC 2119 keywords section, etc.). The second part is created by a script that generates the actual list of terms from the current terms files for term lists that are included in the listing. The script is relatively simple if all terms are found in a single term list. It is more complex if the vocabulary includes terms from several term lists or if the terms are categorized. There are two example build scripts that can be modified by a Python programmer if modifications are needed to make the term list document conform to the idiosyncrasies of a given vocabulary.

## 4.1 Building a simple term list

The notebook `build-page-simple.ipynb` in the `process/page_build_scripts` directory of the rs.tdwg.org repository has an example set up for a controlled vocabulary with hierarchy. That directory also has a template Markdown file for the introductory section that can be modified as necessary.

## 4.2 Categorizing terms

It is reasonable to include the few terms of a simple vocabulary in a single section. However, documents listing the terms of larger and more complicated vocabularies may need to be organized into categories to make it easier to locate related terms. This approach was first used with Darwin Core and has also been adopted by Audubon Core. 

The key to organizing the terms in this way is by using the property `tdwgutility:organizedInClass` where the value is a class under which the subject is organized. NOTE: the local name of this property should not mislead users to think that grouping property terms in this way indicates that the grouped properties have been declared to have the organizing class as a domain. TDWG-minted terms SHOULD NOT have ranges or domains as part of their basic metadata.

In many cases, the organizing class will be a well-known class previously defined by TDWG or some other organization. Examples in Darwin Core are `dwc:Occurrence` and `dcterms:Location`. However, it is also possible to create a "convenience" class within the `tdwgutility:` namespace solely for the purpose of organizing related terms. For example, Audubon Core uses the class `tdwgutility:ResourceCreation` to group property terms related to the creation of multimedia resources. Terms in the `tdwgutility:` namespace are not generally governed by any standard, so organizational class terms can be added as necessary without going through any official change process.

### 4.2.1 Using categories

In order to use categories, edit the configuration section of the build script so that the value of `organized_in_categories` is `True`. Then create Python lists containing corresponding values for `display_order`, `display_labels`, `display_commnets`, and `display_id`. When the script builds the page, it will use these data to organize the terms and create appropriate section headings and notes for the categories. See the notebook `build-page-categories.ipynb` in the `process/page_build_scripts` directory of the rs.tdwg.org repository for an example.

# 5 Managing documents metadata via Python script

After the vocabulary-related metadata are updated, metadata about human-readable documents MUST also be updated. Typically, this will be one or more list of terms documents that make it possible for humans to read the normative content related to the vocabulary terms. The metadata about each list of terms document involved MUST be updated one at a time, using [a Python script](https://github.com/tdwg/rs.tdwg.org/blob/master/process/document_metadata_processing/tdwg_docs_metadata_update.py) that updates the document-related CSV files in the rs.tdwg.org repository.  

Before using the script, the rs.tdwg.org repository SHOULD be cloned to a local file system. The required configuration variables are in the file `general_configuration.yaml`, which MUST be in the same directory as the script. The following variables are included in the file:

- `versionDate` (required). This date SHOULD match the version date for the corresponding vocabulary version.
- `docIri` (required). This is the permanent current IRI assigned to the document following the IRI patterns established [here](http://rs.tdwg.org/index#2-iri-patterns). If this IRI does not match any existing document IRI in the documents metadata, the script treats the document as new, so if the document is a new version of an existing document, it's very important to get this IRI exactly right. The convention is that these documents have trailing slashes.
- `mediaType`. If omitted, `text/markdown` is assumed.
- `lastVersionAccessUri`. Omit for new documents. For existing documents, this is the path to the raw page source of the previous version of the document. If omitted, the previous value will remain unchanged.
- `standardIri` (required). This is the permanent IRI assigned to the standard of which the document is a part. The convention is that these IRIs do not have trailing slashes.

If the document is new, it is REQUIRED that the `authors_configuration.yaml` and `document_configuration.yaml` files be present in the same directory as the script. There are example files in the `example_config_files` subdirectory of the directory that contains the script. The last cells in the script notebook can also be used to generate these two configuration files with values based on any existing standards document. 

If the document is a new version of an existing document, these two files are OPTIONAL.

If `authors_configuration.yaml` is not present in the directory, the data from the previous version will be used. All fields in this file will be used to update the data table and if any of them are missing, those data will be missing in the metadata. All fields are REQUIRED except `affiliation` and `affiliation_uri`. Care should be taken to use exactly the same string for `contributor_role` as previously used in other rows unless the role is actually unique. 

If the `document_configuration.yaml` is not present in the directory, the data from the previous version will be used, except for the `doc_modified` field, which is ignored and is given the value found in the general configuration file. For existing documents, providing values for fields is OPTIONAL. Any fields lacking values with be given the value from the most recent version. Any fields with values will replace previous values. NOTE: since the citation contains the current version date, it SHOULD be updated. Therefore, this file SHOULD be included in the directory.

**IMPORTANT NOTE** These instructions assume that the document that is being updated is a list of terms document. If the document is an new version of some other standards document, and that document's metadata is being updated in the absence of a previous vocabulary update, the script will not generate a new version of the standard, including the metadata about all of the parts of the new standard version. Those rows will have to be created manually. If the non-list of terms document is being created or updated at the same times as a list of terms document, the script will correctly modify the parts of the previously-created new standard version to include the new version of the document. 

## 5.1 Redirection for human-readable term metadata during content negotiation

For term IRIs, redirection during content-negotiation for machine-readable representations is handled automatically by the server, since those representations are generated directly from the metadata stored in the rs.tdwg.org repo. However, the human-readable representations redirect to fragment identifiers in list of terms documents. For currently maintained vocabularies, these are usually GitHub Pages-generated web pages whose actual page URLs do not correspond to the term IRIs. Correct redirection is controlled by the redirect URL in the `redirects.csv` file in the `html` directory of the `rs.tdwg.org`. The data in this table will need to be updated if the actual URL of the list of pages document changes.

# 6 Generating JSON-LD for controlled vocabularies

In order to make controlled vocabularies as widely available as possible, multi-lingual translations of the term labels and definitions should be made available in as many languages as possible. A Python script (build-json-ld.ipynb) to generate JSON-LD is avaialable in the `cv_json_ld` directory. It can be run from any location, so maintenance groups should use it to generate JSON-LD representations of their controlled vocabularies on their own sites. This JSON-LD can then be used by developers to create multilingual tools to make it easier for users to select the right concept and acquire the controlled value string or IRI associated with that concept.

Because the JSON-LD can easily be ingested, it can also be used to build multilingual web applications. Some Javascript code and an HTML file for a simple web page is also available in the directory. To see the page in action, visit [this page](https://heardlibrary.github.io/digital-scholarship/lod/json_ld_test/display-cv.html).
